{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC/Tbh+1JByO0DQzGv7q4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasunamishra/5CS037/blob/main/Week6_Prasuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Task To Do:"
      ],
      "metadata": {
        "id": "G5hedmDArk3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Sigmoid Function:"
      ],
      "metadata": {
        "id": "kd-Bqcv_roi4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2Q4oeeHogBhB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def logistic_function(x):\n",
        "  \"\"\"\n",
        "Computes the logistic function applied to any value of x.\n",
        "Arguments:\n",
        "x: scalar or numpy array of any size.\n",
        "Returns:\n",
        "y: logistic function applied to x.\n",
        "\"\"\"\n",
        "  y = 1 / (1 + np.exp(-x))\n",
        "  return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case for logistic function:"
      ],
      "metadata": {
        "id": "-tO6ILmnsIOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def test_logistic_function():\n",
        " \"\"\"\n",
        "Test cases for the logistic_function.\n",
        " \"\"\"\n",
        "# Test with scalar input\n",
        "x_scalar = 0\n",
        "expected_output_scalar = round(1 / (1 + np.exp(0)), 3) # Expected output: 0.5\n",
        "assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
        "# Test with positive scalar input\n",
        "x_pos = 2\n",
        "expected_output_pos = round(1 / (1 + np.exp(-2)), 3) # Expected output: ~0.881\n",
        "assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
        "# Test with negative scalar input\n",
        "x_neg = -3\n",
        "expected_output_neg = round(1 / (1 + np.exp(3)), 3) # Expected output: ~0.047\n",
        "assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
        "# Test with numpy array input\n",
        "x_array = np.array([0, 2, -3])\n",
        "expected_output_array = np.array([0.5, 0.881, 0.047]) # Adjusted expected values rounded to 3 decimals\n",
        "# Use np.round to round the array element-wise and compare\n",
        "assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
        "print(\"All tests passed!\")\n",
        "# Run the test case\n",
        "test_logistic_function()\n"
      ],
      "metadata": {
        "id": "VnBkhdXorxV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18cdb572-6504-4644-8b63-860ff1e7ca6c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Task To Do:"
      ],
      "metadata": {
        "id": "LYRpcnrMxExa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of log - loss function:"
      ],
      "metadata": {
        "id": "yvDKpXmmLKCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "Computes log loss for true target value y ={0 or 1} and predicted target value yâ€™ inbetween {0-1}.\n",
        "Arguments:\n",
        "y_true (scalar): true target value {0 or 1}.\n",
        "y_pred (scalar): predicted taget value {0-1}.\n",
        "Returns:\n",
        "loss (float): loss/error value\n",
        "\"\"\"\n",
        "  import numpy as np\n",
        "# Ensure y_pred is clipped to avoid log(0)\n",
        "  y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "  loss = -(y_true * np.log(y_pred)) - ((1 - y_true) * np.log(1 - y_pred))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "6ydZQCziLFej"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying the Intution:"
      ],
      "metadata": {
        "id": "FsErihYpMe2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function:\n",
        "y_true, y_pred = 0, 0.1\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
        "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
        "y_true, y_pred = 1, 0.9\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSRL_EgEL5J0",
        "outputId": "5f52b088-d310-493c-eee7-6eb7877e1ab2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log loss(0, 0.1) ==> 0.10536051565782628\n",
            "+++++++++++++--------------------------++++++++++++++++++++++++\n",
            "log loss(1, 0.9) ==> 0.10536051565782628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case for log - loss function:"
      ],
      "metadata": {
        "id": "-0J3z_VlMxiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_log_loss():\n",
        "  \"\"\"\n",
        "Test cases for the log_loss function.\n",
        "\"\"\"\n",
        "  import numpy as np\n",
        "# Test case 1: Perfect prediction (y_true = 1, y_pred = 1)\n",
        "  y_true = 1\n",
        "  y_pred = 1\n",
        "  expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction ( y_true=1, y_pred=1)\"\n",
        "# Test case 2: Perfect prediction (y_true = 0, y_pred = 0)\n",
        "  y_true = 0\n",
        "  y_pred = 0\n",
        "  expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction ( y_true=0, y_pred=0)\"\n",
        "# Test case 3: Incorrect prediction (y_true = 1, y_pred = 0)\n",
        "  y_true = 1\n",
        "  y_pred = 0\n",
        "  try:\n",
        "    log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "  except ValueError:\n",
        "    pass # Test passed if ValueError is raised for log(0)\n",
        "# Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)\n",
        "  y_true = 0\n",
        "  y_pred = 1\n",
        "  try:\n",
        "    log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "  except ValueError:\n",
        "    pass # Test passed if ValueError is raised for log(0)\n",
        "# Test case 5: Partially correct prediction\n",
        "  y_true = 1\n",
        "  y_pred = 0.8\n",
        "  expected_loss = -(1 * np.log(0.8)) - (0 * np.log(0.2)) # ~0.2231\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=1, y_pred=0.8)\"\n",
        "  y_true = 0\n",
        "  y_pred = 0.2\n",
        "  expected_loss = -(0 * np.log(0.2)) - (1 * np.log(0.8)) # ~0.2231\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=0, y_pred=0.2)\"\n",
        "  print(\"All tests passed!\")\n",
        "# Run the test case\n",
        "test_log_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FeP50HgMyCi",
        "outputId": "92088c3d-b6b5-452c-e923-e7f944b2a9f7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Task To Do:"
      ],
      "metadata": {
        "id": "5zZxxxStxODc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemenetation of Cost Function:"
      ],
      "metadata": {
        "id": "NikjNuhRN0aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(y_true, y_pred):\n",
        "  \"\"\"\n",
        "Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "Args:\n",
        "y_true (array_like, shape (n,)): array of true values (0 or 1)\n",
        "y_pred (array_like, shape (n,)): array of predicted values (probability of y_pred being 1)\n",
        "Returns:\n",
        "cost (float): nonnegative cost corresponding to y_true and y_pred\n",
        "\"\"\"\n",
        "  assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "  n = len(y_true)\n",
        "  loss_vec =[log_loss(y_true[i], y_pred[i]) for i in range(n)]\n",
        "  cost = sum(loss_vec) / n\n",
        "  return cost"
      ],
      "metadata": {
        "id": "nEYZNjuhM1Gj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Cost Function:"
      ],
      "metadata": {
        "id": "OrkUV00CObH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def test_cost_function():\n",
        "# Test case 1: Simple example with known expected cost\n",
        "  y_true = np.array([1, 0, 1])\n",
        "  y_pred = np.array([0.9, 0.1, 0.8])\n",
        "# Expected output: Manually calculate cost for these values\n",
        "# log_loss(y_true, y_pred) for each example\n",
        "  expected_cost = (-(1 * np.log(0.9)) - (1 - 1) * np.log(1 - 0.9) +\n",
        "                  -(0 * np.log(0.1)) - (1 - 0) * np.log(1 - 0.1) +\n",
        "                  -(1 * np.log(0.8)) - (1 - 1) * np.log(1 - 0.8)) / 3\n",
        "\n",
        "# Call the cost_function to get the result\n",
        "  result = cost_function(y_true, y_pred)\n",
        "# Assert that the result is close to the expected cost with a tolerance of 1e-6\n",
        "  assert np.isclose(result, expected_cost, atol=1e-6), f\"Test failed: {result} != {expected_cost}\"\n",
        "  print(\"Test passed for simple case!\")\n",
        "# Run the test case\n",
        "test_cost_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYDELXpOYxG",
        "outputId": "f2eed3a1-d743-446d-8f43-77f9172f7370"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed for simple case!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Task To Do:"
      ],
      "metadata": {
        "id": "vLwdJAGOxc9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Cost Function for Logistic/Sigmoid Regression:"
      ],
      "metadata": {
        "id": "-zDubZKdOqls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute cost function in terms of model parameters - using vectorization\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "  \"\"\"\n",
        "Computes the cost function, given data and model parameters.\n",
        "Args:\n",
        "X (ndarray, shape (m,n)): data on features, m observations with n features.\n",
        "y (array_like, shape (m,)): array of true values of target (0 or 1).\n",
        "w (array_like, shape (n,)): weight parameters of the model.\n",
        "b (float): bias parameter of the model.\n",
        "Returns:\n",
        "cost (float): nonnegative cost corresponding to y and y_pred.\n",
        "\"\"\"\n",
        "  n, d = X.shape\n",
        "  assert len(y) == n, \"Number of feature observations and number of target observations do not match.\"\n",
        "  assert len(w) == d, \"Number of features and number of weight parameters do not match.\"\n",
        "# Compute z using np.dot\n",
        "  z = np.dot(X, w) + b # Matrix-vector multiplication and adding bias\n",
        "# Compute predictions using logistic function (sigmoid)\n",
        "  y_pred = logistic_function(z)\n",
        "# Compute the cost using the cost function\n",
        "  cost = cost_function(y, y_pred)\n",
        "  return cost\n",
        "# Testing the Function:\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_6U09GLOtlQ",
        "outputId": "c30a7053-da39-4c80-f9ca-764e66e1589e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost for logistic regression(X = [[ 10  20]\n",
            " [-10  10]], y = [1 0], w = [0.5 1.5], b = 1) = 5.500008350834906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Task To Do:"
      ],
      "metadata": {
        "id": "VkLs2eXyxqIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Gradients for Sigmoid Regression:"
      ],
      "metadata": {
        "id": "2f72W8k6POrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "  \"\"\"\n",
        "Computes gradients of the cost function with respect to model parameters.\n",
        "Args:\n",
        "X (ndarray, shape (n,d)): Input data, n observations with d features\n",
        "y (array_like, shape (n,)): True labels (0 or 1)\n",
        "w (array_like, shape (d,)): Weight parameters of the model\n",
        "b (float): Bias parameter of the model\n",
        "Returns:\n",
        "grad_w (array_like, shape (d,)): Gradients of the cost function with respect to the weight\n",
        "parameters\n",
        "grad_b (float): Gradient of the cost function with respect to the bias parameter\n",
        "\"\"\"\n",
        "  n, d = X.shape # X has shape (n, d)\n",
        "  assert len(y) == n, f\"Expected y to have {n} elements, but got {len(y)}\"\n",
        "  assert len(w) == d, f\"Expected w to have {d} elements, but got {len(w)}\"\n",
        "# Compute predictions using logistic function (sigmoid)\n",
        "  z = np.dot(X, w) + b\n",
        "  y_pred = logistic_function(z) # Compute z = X * w + b\n",
        "# Compute gradients\n",
        "  grad_w = -(1 / n) * np.dot(X.T, (y - y_pred)) # Gradient w.r.t weights, shape (d,)\n",
        "  grad_b = -(1 / n) * np.sum(y - y_pred) # Gradient w.r.t bias, scalar\n",
        "  return grad_w, grad_b"
      ],
      "metadata": {
        "id": "IM_DRjsUPSbI"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple assertion test for compute gradient function:"
      ],
      "metadata": {
        "id": "Hg7IfM1MP06a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple test case\n",
        "X = np.array([[10, 20], [-10, 10]]) # shape (2, 2)\n",
        "y = np.array([1, 0]) # shape (2,)\n",
        "w = np.array([0.5, 1.5]) # shape (2,)\n",
        "b = 1 # scalar\n",
        "# Assertion tests\n",
        "try:\n",
        "  grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "  print(\"Gradients computed successfully.\")\n",
        "  print(f\"grad_w: {grad_w}\")\n",
        "  print(f\"grad_b: {grad_b}\")\n",
        "except AssertionError as e:\n",
        "  print(f\"Assertion error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mGyYcwIP27J",
        "outputId": "0de4f7ec-4194-4db1-ceb9-0a3b1ebb24ce"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients computed successfully.\n",
            "grad_w: [-4.99991649  4.99991649]\n",
            "grad_b: 0.4999916492890759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Task To Do:"
      ],
      "metadata": {
        "id": "uDIyct7dxyKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent for Sigmoid Regression:"
      ],
      "metadata": {
        "id": "4X1n74MsR08f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "  \"\"\"\n",
        "Implements batch gradient descent to optimize logistic regression parameters.\n",
        "Args:\n",
        "X (ndarray, shape (n,d)): Data on features, n observations with d features\n",
        "y (array_like, shape (n,)): True values of target (0 or 1)\n",
        "w (array_like, shape (d,)): Initial weight parameters\n",
        "b (float): Initial bias parameter\n",
        "alpha (float): Learning rate\n",
        "n_iter (int): Number of iterations\n",
        "show_cost (bool): If True, displays cost every 100 iterations\n",
        "show_params (bool): If True, displays parameters every 100 iterations\n",
        "Returns:\n",
        "w (array_like, shape (d,)): Optimized weight parameters\n",
        "b (float): Optimized bias parameter\n",
        "cost_history (list): List of cost values over iterations\n",
        "params_history (list): List of parameters (w, b) over iterations\n",
        "\"\"\"\n",
        "  n, d = X.shape\n",
        "  assert len(y) == n, \"Number of observations in X and y do not match\"\n",
        "  assert len(w) == d, \"Number of features in X and w do not match\"\n",
        "  cost_history = []\n",
        "  params_history = []\n",
        "  for i in range(n_iter):\n",
        "# Compute gradients\n",
        "    grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "# Update weights and bias\n",
        "    w -= alpha * grad_w\n",
        "    b -= alpha * grad_b\n",
        "# Compute cost\n",
        "    cost = costfunction_logreg(X, y, w, b)\n",
        "# Store cost and parameters\n",
        "    cost_history.append(cost)\n",
        "    params_history.append((w.copy(), b))\n",
        "# Optionally print cost and parameters\n",
        "  if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "    print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "  if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
        "    print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
        "  return w, b, cost_history, params_history\n",
        "# Test the gradient_descent function with sample data\n",
        "X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "y = np.array([1, 0]) # Shape (2,)\n",
        "w = np.zeros(X.shape[1]) # Shape (2,) - same as number of features\n",
        "b = 0.0 # Scalar\n",
        "alpha = 0.1 # Learning rate\n",
        "n_iter = 100000 # Number of iterations\n",
        "# Perform gradient descent\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=True,show_params=False)\n",
        "# Print final parameters and cost\n",
        "print(\"\\nFinal parameters:\")\n",
        "print(f\"w: {w_out}, b: {b_out}\")\n",
        "print(f\"Final cost: {cost_history[-1]:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyb0YYe3RiB2",
        "outputId": "8a7636b9-b271-4971-8da4-23ad85e23401"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 99999: Cost = 0.008254\n",
            "\n",
            "Final parameters:\n",
            "w: [38.51304248 18.83386869], b: -2.8176836626325836\n",
            "Final cost: 0.008254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple assertion test for gradient descent Function:"
      ],
      "metadata": {
        "id": "fG9ju36pT4YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple assertion test for gradient_descent\n",
        "def test_gradient_descent():\n",
        "  X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "  y = np.array([1, 0]) # Shape (2,)\n",
        "  w = np.zeros(X.shape[1]) # Shape (2,)\n",
        "  b = 0.0 # Scalar\n",
        "  alpha = 0.1 # Learning rate\n",
        "  n_iter = 100 # Number of iterations\n",
        "# Run gradient descent\n",
        "  w_out, b_out, cost_history, _ = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False,show_params=False)\n",
        "# Assertions\n",
        "  assert len(cost_history) == n_iter, \"Cost history length does not match the number of iterations\"\n",
        "  assert w_out.shape == w.shape, \"Shape of output weights does not match the initial weights\"\n",
        "  assert isinstance(b_out, float), \"Bias output is not a float\"\n",
        "  assert cost_history[-1] < cost_history[0], \"Cost did not decrease over iterations\"\n",
        "  print(\"All tests passed!\")\n",
        "# Run the test\n",
        "test_gradient_descent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzyTJ9iwTzkE",
        "outputId": "a7a6b1fb-3435-43a8-8276-d6a0420eaaef"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Task To Do:"
      ],
      "metadata": {
        "id": "o7aDUQwnyPQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision/Prediction Function:"
      ],
      "metadata": {
        "id": "kOcrnUN3vGZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def prediction(X, w, b, threshold=0.5):\n",
        "  \"\"\"\n",
        "Predicts binary outcomes for given input features based on logistic regression parameters.\n",
        "Arguments:\n",
        "X (ndarray, shape (n,d)): Array of test independent variables (features) with n samples and d\n",
        "features.\n",
        "w (ndarray, shape (d,)): Array of weights learned via gradient descent.\n",
        "b (float): Bias learned via gradient descent.\n",
        "threshold (float, optional): Classification threshold for predicting class labels. Default is 0.5.\n",
        "Returns:\n",
        "y_pred (ndarray, shape (n,)): Array of predicted dependent variable (binary class labels: 0 or 1).\n",
        "\"\"\"\n",
        "# Compute the predicted probabilities using the logistic function\n",
        "  z = np.dot(X, w) + b\n",
        "  y_test_prob =logistic_function(z) # z = wx + b\n",
        "# Classify based on the threshold\n",
        "  y_pred =(y_test_prob >= threshold).astype(int)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "_V7hP4YRT2pU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple assertion test for Prediction Function:"
      ],
      "metadata": {
        "id": "0DKlYvUpviTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prediction():\n",
        "  X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]]) # Shape (3, 2)\n",
        "  w_test = np.array([1.0, -1.0]) # Shape (2,)\n",
        "  b_test = 0.0 # Scalar bias\n",
        "  threshold = 0.5 # Default threshold\n",
        "# Updated expected output\n",
        "  expected_output = np.array([0, 1, 1])\n",
        "# Call the prediction function\n",
        "  y_pred = prediction(X_test, w_test, b_test, threshold)\n",
        "# Assert that the output matches the expected output\n",
        "  assert np.array_equal(y_pred, expected_output), f\"Expected {expected_output}, but got {y_pred}\"\n",
        "  print(\"Test passed!\")\n",
        "test_prediction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U5KdAOKvi9s",
        "outputId": "3eb5fbff-3c0c-43a1-813d-1041449b9d7e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Task To Do:"
      ],
      "metadata": {
        "id": "_KLofrJByUz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of the Classifier:"
      ],
      "metadata": {
        "id": "Za5nMV9tv1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(y_true, y_pred):\n",
        "  \"\"\"\n",
        "Computes the confusion matrix, precision, recall, and F1-score for binary classification.\n",
        "Arguments:\n",
        "y_true (ndarray, shape (n,)): Ground truth binary labels (0 or 1).\n",
        "y_pred (ndarray, shape (n,)): Predicted binary labels (0 or 1).\n",
        "Returns:\n",
        "metrics (dict): A dictionary containing confusion matrix, precision, recall, and F1-score.\n",
        "\"\"\"\n",
        "# Initialize confusion matrix components\n",
        "  TP = np.sum((y_true == 1) & (y_pred == 1)) # True Positives\n",
        "  TN = np.sum((y_true == 0) & (y_pred == 0)) # True Negatives\n",
        "  FP = np.sum((y_true == 0) & (y_pred == 1)) # False Positives\n",
        "  FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "# Confusion matrix\n",
        "  confusion_matrix = np.array([[TN, FP],\n",
        "                      [FN, TP]])\n",
        "# Precision, recall, and F1-score\n",
        "  precision = TP / (TP + FP) if (TP + FP) > 0.0 else 0.0\n",
        "  recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "  f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "# Metrics dictionary\n",
        "  metrics = {\n",
        "            \"confusion_matrix\": confusion_matrix,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1_score\n",
        "            }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "gG1fS1tqv1Wa"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}